## Luisa Fernanda Restrepo Perdomo

## Periodismo de Datos II: Herramientas Digitales para la Visualización y Presentación de Datos

## Metodología 

En este documento realizado en Jupyter llamado "metodologia" explicaré el **procedimiento de los trabajos y las tareas realizadas durante la asignatura**, para conseguir una web con la funcionalidad Pages.

A lo largo del curso hicimos cuatro actividades dirigidas y este, el trabajo final, utilizando el lenguaje Markdown. Estos ejercicios nos permitieron aprender a leer codigo de Python usando la aplicación Jupyter, el repositorio de GitHub y algunas librerías como Pandas. 

Paso a explicar cada una de estas, pues fueron las herramientas que más utilizamos durante la asignatura:

**GitHub:** es un portal creado como repositorio, es decir, nos permite alojar el código de las aplicaciones de cualquier desarrollador. Aprendimos que no solo puedes visualizar tus archivos, sino también puedes entrar a todos los perfiles para leer y colaborar con su desarrollo. Este portal usa un sistema de control de versiones llamado Git, con este cada usuario puede administrar su proyecto, ordenando el código de cada una de las versiones para evitar confusiones. 
En este repositorio alojamos todas las actividades de la asignatura y las pudimos visualizar en la web con Pages.

**Jupyter:** es una aplicación que sirve como cuaderno de notas y que nos permitió crear y compartir documentos web en formato "j_son". Esta sigue un esquema versionado y una lista ordenada de celdas de entrada y de salida. Es muy útil porque en ella se puede escribir como programación literaria en Markdown y en código.  

**Pandas:** es una librería de código abierto que esta dentro de los desarrolladores de Python. Esta facilita la lectura y el tratamiento de los datos dentro de una web.

Este documento llamado "metodologia" es un ejercicio explicativo de los pasos que hemos seguido para cada actividad. Intenta ser un ejercicio de programación literaria, es decir, que está escrito con bloques de texto y de código que explican los procesos. 

Acá detallaré los pasos de cada una de las actividades para mostrar lo aprendido en la asignatura.

### Actividad 1 (ad1)

Esta actividad consistió en hacer un **comentario crítico sobre una pieza de periodismo de datos, visualización de datos o infografía.** 

Para hacer el anális usamos lo aprendido en Periodismo de Datos I. [La pieza analizada fue esta.](https://www.elcolombiano.com/multimedia/infografias/las-prioridades-para-contener-avance-del-covid-19-BF12609228). Es un reportaje sobre los esfuerzos de los paises para contener el avance del COVID19.

Esta actividad fue desafiante pues no conocía el lenguaje Markdown y nunca había oido hablar de GitHub.

Usamos el "pad" riseup, una aplicación web que nos permitió escribir el texto de forma colaborativa. Cada uno escribió su texto en lenguaje Markdown, un lenguaje que se asemeja al "código fuente" de un texto y que se utiliza para componer páginas web, cuadernos de Python y para gráficos interactivos. También sirve para la transformación a cualquier formato estructurado como PDF, docx o HTML de los contenidos. 

**El codigo de Markdown nos permitió hacer:**

Encabezados de diferente nivel con almohadillas #.

Negrita, utilizando 2 asteriscos **al principio y al final del texto**.

Enlaces, con el texto a enlazar entre corchetes y la url entre paréntesis.

Eso al principio de la actividad, luego, aprendimos sobre el repositorio GitHub, que es un portal usado para alojar el código de las aplicaciones de cualquier desarrollador como explique en la introducción. En este guardamos el contenido que elaboramos en lenguaje Markdown y todos los usuarios registrado en nebrijas2022 podían verlo. 

Cuando estuve más familiariazada, pude añadir en el GitHub imágenes con atributos alt desde la url de origen. En este caso añadí la infografía como una imagen desde la url poniendola entre paréntesis precedida por el atributo alt entre corchetes y el signo de admiración final.

Gracias a esta actividad pude aprender sobre el lenguaje Markdowmn y conocer un modo fácil de construir una página web desde un repositorio online que tiene muchas funcionalidades.

**La ad1 se puede ver en [este enlace del repositorio de GitHub.](https://github.com/nebrijas/2022online-luisafrp/blob/main/ad1.md)**


El siguiente paso consistió en que **cada uno, desde su usuario en GitHub, creó una página web sencilla compuesta por las carpetas de cada una de las actividades que desarrollamos durante el curso**, nombrándolas de la misma forma pues es importante cuidar la unidad en la escritura web para facilitar su ejecución.

Estos contenidos están disponibles y se pueden visualizar online gracias a Pages. Esta aplicación te permite publicar el código del sitio en vivo en la web.

### Actividad 2 (ad2)

Esta actividad consistió en realizar un **comentario de un artículo o reportaje de periodismo y visualización de datos.** También fue escrito en Markdown y subido al repositorio de GitHub.

En este caso fue, [este reportaje sobre los mundiales](https://www.eltiempo.com/datos/mundial-qatar-2022-datos-curiosos-de-la-copa-mundo-de-la-fifa-711095), que contiene un gráfico sobre los estadios en los que más se han jugado partidos en los mundiales. Este me llamó mucho la atención por la forma como está hecho, pues se visualiza de manera muy original y clara.

En este ejercicio ya estaba más familiarizada con el lenguaje Markdown y con el GitHub, entonces fue más fácil realizarlo.

Añadí la infografía que analicé, precedida por un atributo alt que explica el contenido de la imagen y el signo de admiración final al inicio del código.

El comentario estuvo relacionado a lo aprendido en las clases sobre la forma correcta de hacer visualizaciones y el uso correcto de los datos. 

**La ad2 se puede ver en [este enlace del repositorio de GitHub.](https://github.com/nebrijas/2022online-luisafrp/blob/main/ad2.md)**

### Actividad 3 (ad3)

Acá transformamos un ejercicio de código de Python para lograr un scraping de una web. Esto significa sacar datos mediante algunas herramientas o librerías que leen el código. 

En esta actividad profundizamos un poco más en las herramientas para el análisis y la visualización de datos. Lo primero que hicimos fue instalar Anaconda, que es una distribución libre y abierta de los lenguajes Python y R. Esta permite el procesamiento de grandes volúmenes de información y tiene como ventaja simplificar la gestión e implementación de paquetes. 

Una vez instalado Anaconda, pudimos acceder desde allí a Jupyter, una aplicación web que permite generar cuadernos de Python y Markdown. Desde Jupyter comenzamos a hacer un **ejercicio de programación literaria.**

El archivo que desglosamos fue un código de Python (py) y comentamos el paso a paso de este desglose. 

En este ejercicio aprendí que hay varias librerías que nos permiten leer código Python. Entre ellas están: Pandas, Request, Termcolor, BeautifulSoup, entre otras. Algunas de estas liberías son externas y otras vienen con la herramienta Anaconda que anteriormente nos habíamos descargado a nuestro computador.

Me pareció un ejercicio muy útil para extraer información muy específica por medio de etiquetas o "tags". La idea fue lograr un ejercicio en el que entendieramos lo que estábamos haciendo y luego, si lo veíamos después de un tiempo, lo pudieramos leer de igual forma. Es como un hilo argumental en un cuaderno de notas.

Lo más importante de la actividad fue descargar las librerías y ponerlas a correr para luego ejecutar las acciones en orden a obtener los datos.

**La ad3 se puede ver [en este respositorio de GitHub.](https://github.com/nebrijas/2022online-luisafrp/blob/main/ad3.md)**

Para hacer el scraping comenzamos instalando las librerías que no están incluidas dentro de Anaconda usando !pip que es el programa de instalación de Python. El código que usamos fue el siguiente:


```python
!pip install requests bs4 pandas termcolor
```

    Requirement already satisfied: requests in c:\users\luisa\anaconda3\lib\site-packages (2.28.1)
    Requirement already satisfied: bs4 in c:\users\luisa\anaconda3\lib\site-packages (0.0.1)
    Requirement already satisfied: pandas in c:\users\luisa\anaconda3\lib\site-packages (1.4.4)
    Requirement already satisfied: termcolor in c:\users\luisa\anaconda3\lib\site-packages (2.1.1)
    Requirement already satisfied: certifi>=2017.4.17 in c:\users\luisa\anaconda3\lib\site-packages (from requests) (2022.9.14)
    Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\users\luisa\anaconda3\lib\site-packages (from requests) (1.26.11)
    Requirement already satisfied: idna<4,>=2.5 in c:\users\luisa\anaconda3\lib\site-packages (from requests) (3.3)
    Requirement already satisfied: charset-normalizer<3,>=2 in c:\users\luisa\anaconda3\lib\site-packages (from requests) (2.0.4)
    Requirement already satisfied: beautifulsoup4 in c:\users\luisa\anaconda3\lib\site-packages (from bs4) (4.11.1)
    Requirement already satisfied: numpy>=1.18.5 in c:\users\luisa\anaconda3\lib\site-packages (from pandas) (1.21.5)
    Requirement already satisfied: python-dateutil>=2.8.1 in c:\users\luisa\anaconda3\lib\site-packages (from pandas) (2.8.2)
    Requirement already satisfied: pytz>=2020.1 in c:\users\luisa\anaconda3\lib\site-packages (from pandas) (2022.1)
    Requirement already satisfied: six>=1.5 in c:\users\luisa\anaconda3\lib\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)
    Requirement already satisfied: soupsieve>1.2 in c:\users\luisa\anaconda3\lib\site-packages (from beautifulsoup4->bs4) (2.3.1)
    

Cada librería tiene un uso particular dentro del scraping. Acá explico cada una:

**Librerías que vienen instaladas:**

- **CSV:** (valores separados por comas). Es el formato habitual de importación y exportación de hojas de cálculo y bases de datos.

- **Re:** esta nos ayuda para la coincidencia de expresiones regulares similares a las encontradas en Perl.

- **Os:** nos ayuda a usar funciones del sistema operativo.

- **Time:** tiene varias funciones relacionadas con el tiempo. 

**Librerías que hay que instalar:**

- **Requests:** se usa para hacer solicitudes y peticiones a la página de la que extraeremos los datos.

- **Pandas:** es una herramienta de análisis de datos de código en el lenguaje de Python.

- **Termcolor:** sirve para imprimir mensajes de colores en el terminal.

- **BeautifulSoup** sirve para obtener datos en formato HTML y XML.

Luego buscamos obtener los resultados. Para esto llamamos a la libería Request que nos ayudó a **descargar los resultados de la página de El País en sus distintas secciones.** Y así sucesivamente llamamos a las otras librerías para extraer los datos que necesitabamos, en este caso lo títulos h2. 

El código que usamos fue el siguiente:


```python
req = requests.get("https://resultados.elpais.com")
# Si el estatus code no es 200 no se puede leer la página
if (req.status_code != 200):
 raise Exception("No se puede hacer Web Scraping en"+ URL)
soup = BeautifulSoup(req.text, 'html.parser')
```

Con el código correcto se le puede ir dando valor a las distintas variables para que extraigan cualquier tipo de datos como palabras, títulos, listas, números, etc. 

Ahora le pedimos a la librería BeautifulSoup, que es una biblioteca de la cual podemos obtener datos en formato HTML y XML, que encuentre y en este caso todas las etiquetas "h2":


```python
tags = soup.findAll("h2")
```

Luego que encuentre y que imprima la información que le pedimos:


```python
for h2 in tags:
    print(h2.text)
    resultados.append(h2.text)
```

Al finalizar la actividad descargamos del Jupyter dos archivos uno en formato Jupyter y otro en formato Markdown para subirlos  respectivamente al GitHub y al campusvirtual de Nebrija.

### Actividad 4 (ad4)

La actividad consistió en **conectar desde Jupyter a la API de datos del COVID19** a la que se puede [acceder aquí](https://covid19api.com).

Fue la actividad a mi modo de ver más interesante porque pudimos elaborar gráficos con datos sobre la pandemia de varios paises como Colombia, Ecuador, República Dominicana y España. Además hicimos el recorrido literario de cada una de las acciones.

Acá seguimos pasos similares a los de la actividad 3: instalamos las librerías, le dimos valor a la variable "miurl" para que pudiera extraer los datos que necesitabamos, usamos dataframes para que estos datos se mostraran de forma ordenada y elaboramos gráficos con los datos obtenidos.

En este ejercicio aprendí lo que es un dataframe, que es un marco de datos que sirve para encerrar, organizar e ilustrar la información. A este dataframe se le puede explorar de muchas formas y nos brinda la información que queramos para poder comparar los datos y obtenerlos de forma ordenada para poder hacer gráficos. Acá un ejemplo:

![dataframe](dataframe.jpg)

Asímismo aprendí a sobre la función "j_son", que significa JavaScript Object Notation. Este es un formato basado en texto estándar y se puede usar para representar datos estructurados en la sintaxis JavaScript en forma de texto y transmitirlos de un sistema a otro.

Finalmente, para elaborar un gráfico, objetivo principal de esta actividad, seleccionamos los ejes, en este caso la fecha y el número de casos. Luego lo ilustramos con "plot" de plotear (df_co.set_index('Date')['Cases'].plot()) y por último lo nombramos añadiendo al final la función "title". 

![gráfico COVID19 en Colombia](graficocolombia.jpg)

**La ad4 se puede ver [aquí en el repositorio de GitHub.](https://github.com/nebrijas/2022online-luisafrp/blob/main/ad4.md)**


## Conclusión

En esta asignatura nos enseñó a usar nuevas plataformas web como GitHub, Anaconda y Jupyter, que son herramientas muy útiles que no conocía. También profundicé más sobre el lenguaje Python y conocí el lenguaje Markdown. 
Así mismo profundicé en los códigos fuente, que además de darme conocimientos importantes sobre programación web, me sirvieron para pbtener datos, tratarlos y elaborar visualizaciones.

Ahora la web tiene mucha utilidad en orden a los datos que se pueden obtener de ella para realizar periodismo y poder conocer  toda la información que hay detrás de cada página web.



```python

```
